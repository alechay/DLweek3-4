{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data with DLTK\n",
    "To build a reader you have to define a read function. This is a function with a signature `read_fn(file_references, mode, params=None)`, where \n",
    "- `file_references` is a array_like variable to be used to read files but can also be `None` if not used at all. \n",
    "- `mode` is a mode key from `tf.estimator.ModeKeys` and \n",
    "- `params` is a dictionary or `None` to pass additional parameters, you might need during interfacing with your inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom python generator `read_fn`\n",
    "In the following cell we define an example reader to read from the IXI dataset you can download with \n",
    "the included script. Let's start with some python, before we go into tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/alechay/opt/anaconda3/envs/DLweek3-4/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import os\n",
    "from dltk.io.augmentation import *\n",
    "from dltk.io.preprocessing import *\n",
    "\n",
    "# dltk compatible with tensorflow 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def read_fn(file_references, mode, params=None):\n",
    "    \n",
    "    # We define a `read_fn` and iterate through the `file_references`, which\n",
    "    # can contain information about the data to be read (e.g. a file path):\n",
    "    for meta_data in file_references:\n",
    "        \n",
    "        # Here, we parse the `subject_id` to construct a file path to read\n",
    "        # an image from.\n",
    "        subject_id = meta_data[0]\n",
    "        data_path = '../DLTK/1mm'\n",
    "        t1_fn = os.path.join(data_path, '{}/T1_1mm.nii.gz'.format(subject_id))\n",
    "        \n",
    "        # Read the .nii image containing a brain volume with SimpleITK and get \n",
    "        # the numpy array:\n",
    "        sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "        t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "        # Normalise the image to zero mean/unit std dev:\n",
    "        t1 = whitening(t1)\n",
    "        \n",
    "        # Create a 4D Tensor with a dummy dimension for channels\n",
    "        t1 = t1[..., np.newaxis]\n",
    "        \n",
    "        # If in PREDICT mode, yield the image (because there will be no label\n",
    "        # present). Additionally, yield the sitk.Image pointer (including all\n",
    "        # the header information) and some metadata (e.g. the subject id),\n",
    "        # to facilitate post-processing (e.g. reslicing) and saving.\n",
    "        # This can be useful when you want to use the same read function as \n",
    "        # python generator for deployment. Note: Data are not passed to\n",
    "        # tensorflow if we do not specify a data type for them \n",
    "        # (c.f. `dltk/io/abstract_reader`):\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            yield {'features': {'x': t1},\n",
    "                   'metadata': {\n",
    "                       'subject_id': subject_id,\n",
    "                       'sitk': sitk_t1}}\n",
    "\n",
    "        # Labels: Here, we parse the class *sex* from the file_references \n",
    "        # \\in [1,2] and shift them to \\in [0,1] for training:\n",
    "        sex = np.int32(meta_data[1]) - 1\n",
    "        y = sex\n",
    "            \n",
    "        # If in TRAIN mode, we want to augment the data to generalise better\n",
    "        # and avoid overfitting to the training dataset:\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # Insert augmentation function here (see `dltk/io/augmentation`)\n",
    "            pass\n",
    "        \n",
    "        # If training should be done on image patches for improved mixing, \n",
    "        # memory limitations or class balancing, call a patch extractor \n",
    "        # (see `dltk/io/augmentation`):\n",
    "        if params['extract_examples']:\n",
    "            images = extract_random_example_array(\n",
    "                t1,\n",
    "                example_size=params['example_size'],\n",
    "                n_examples=params['n_examples'])\n",
    "            \n",
    "            # Loop the extracted image patches and yield\n",
    "            for e in range(params['n_examples']):\n",
    "                yield {'features': {'x': images[e].astype(np.float32)},\n",
    "                       'labels': {'y': y.astype(np.int32)},\n",
    "                       'metadata': {\n",
    "                           'subject_id': subject_id,\n",
    "                           'sitk': sitk_t1}}\n",
    "                     \n",
    "        # If desired (i.e. for evaluation, etc.), return the full images\n",
    "        else:\n",
    "            yield {'features': {'x': images},\n",
    "                   'labels': {'y': y.astype(np.int32)},\n",
    "                   'metadata': {\n",
    "                       'subject_id': subject_id,\n",
    "                       'sitk': sitk_t1}}\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_fn` above can be used as generator in python but we wrap it for you with our `Reader` class.\n",
    "For debugging, you can visualise the examples as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features': {'x': array([[[[-0.7190699],\n",
      "         ...,\n",
      "         [-0.7190699]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7190699],\n",
      "         ...,\n",
      "         [-0.7190699]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[-0.7190699],\n",
      "         ...,\n",
      "         [-0.7190699]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7190699],\n",
      "         ...,\n",
      "         [-0.7190699]]]], dtype=float32)}, 'metadata': {'subject_id': 'IXI012', 'sitk': <SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'std::vector< itk::simple::Image >::value_type *' at 0x7fa8701a0f60> >}}\n"
     ]
    }
   ],
   "source": [
    "# Use pandas to read csvs that hold meta information to read the files from disk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "all_filenames = pd.read_csv(\n",
    "    '../DLTK/demographic_HH.csv',\n",
    "    dtype=object,\n",
    "    keep_default_na=False,\n",
    "    na_values=[]).to_numpy()\n",
    "\n",
    "# Set up a some parameters as required in the `read_fn`:\n",
    "reader_params = {'n_examples': 1, \n",
    "                 'example_size': [128, 224, 224],\n",
    "                 'extract_examples': True}\n",
    "\n",
    "# Create a generator with the read file_references `all_filenames` and \n",
    "# `reader_params` in PREDICT mode:\n",
    "it = read_fn(file_references=all_filenames,\n",
    "             mode=tf.estimator.ModeKeys.PREDICT,\n",
    "             params=reader_params)\n",
    "\n",
    "# If you call `next`, the `read_fn` will yield an output dictionary as designed\n",
    "# by you:\n",
    "ex_dict = next(it)\n",
    "\n",
    "# Print that output dict to debug\n",
    "np.set_printoptions(edgeitems=1)\n",
    "print(ex_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a custom `read_fn` with TensorFlow\n",
    "In order to use the `read_fn` in a tensorflow graph, we wrap the generator to feed a Tensorflow Dataset. You can generate this queue using `dltk/io/abstract_reader` or do it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.train' has no attribute 'SessionRunHook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c341191087dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Import and create a dltk reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m reader = Reader(read_fn=read_fn,\n\u001b[1;32m     19\u001b[0m                 dtypes=reader_example_dtypes)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/DLweek3-4/lib/python3.7/site-packages/dltk/io/abstract_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mIteratorInitializerHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSessionRunHook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"Hook to initialise data iterator after Session is created.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.train' has no attribute 'SessionRunHook'"
     ]
    }
   ],
   "source": [
    "# As before, define the desired shapes of the examples and parameters to \n",
    "# pass to your `read_fn`:\n",
    "reader_example_shapes = {'features': {'x': reader_params['example_size'] + [1,]},\n",
    "                         'labels': {'y': []}}\n",
    "\n",
    "reader_params = {'n_examples': 1,\n",
    "                 'example_size': [128, 224, 224],\n",
    "                 'extract_examples': True}\n",
    "\n",
    "# If data_types are set for output dictionary entries, the `dltk/io/abstract_reader`\n",
    "# creates a tensorflow queue and enqueues the respective outputs for training.\n",
    "# Here, we would like to train our features and use labels as targets: \n",
    "reader_example_dtypes = {'features': {'x': tf.float32},\n",
    "                         'labels': {'y': tf.int32}}\n",
    "\n",
    "# Import and create a dltk reader\n",
    "from dltk.io.abstract_reader import Reader\n",
    "reader = Reader(read_fn=read_fn,\n",
    "                dtypes=reader_example_dtypes)\n",
    "\n",
    "# Now, get the input function and queue initialisation hook to use in a `tf.Session` or \n",
    "# with `tf.Estimator`. `shuffle_cache_size` defines the capacity of the queue.\n",
    "input_fn, qinit_hook = reader.get_inputs(all_filenames,\n",
    "                                         tf.estimator.ModeKeys.TRAIN,\n",
    "                                         example_shapes=reader_example_shapes,\n",
    "                                         batch_size=4,\n",
    "                                         shuffle_cache_size=10, \n",
    "                                         params=reader_params)\n",
    "\n",
    "# The input function splits the dictionary of `read_fn` into `features` and `labels` to\n",
    "# match the `tf.Estimator` input requirements. However, both are standard dictionaries.\n",
    "features, labels = input_fn()\n",
    "\n",
    "# Let's create a `tf.Session` and get a batch of features and corresponding labels:\n",
    "s = tf.train.MonitoredTrainingSession(hooks=[qinit_hook])\n",
    "batch_features, batch_labels = s.run([features, labels])\n",
    "\n",
    "# We can visualise the `batch_features` using matplotlib.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(batch_features['x'][0, 0, :, :, 0], 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information on `dltk.io.abstract_reader`:\n",
    "DLTK uses Tensorflow's queueing options to efficiently pass data to the computational graph. Our setup makes use of the [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) API that enables us to use TFs wrappers with [tf.data.Dataset.from_generator](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator). We still wrap this function for better stack traces and to provide input functions suitable for [tf.Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Reader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DLweek3-4)",
   "language": "python",
   "name": "dlweek3-4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
